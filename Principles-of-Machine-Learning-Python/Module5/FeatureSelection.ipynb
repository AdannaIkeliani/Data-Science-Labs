{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Feature Selection\n\n**Feature selection** can be an important part of model selection. In supervised learning, including features in a model which do not provide information on the label is useless at best and may prevent generalization at worst.\n\nFeature selection can involve application of several methods. Two important methods include:\n1. Eliminating features with **low variance** and **zero variance**. Zero variance features are comprised of the same values. Low variance features arise from features with most values the same and with few unique values. One way low variance features can arise, is from dummy variables for categories with very few members. The dummy variable will be mostly 0s with very few 1s. \n2. Training machine learning models with features that are **uninformative** can create a variety of problems. An uninformative feature does not significantly improve model performance. In many cases, the noise in the uninformative features will increase the variance of the model predictions. In other words, uninformative models are likely to reduce the ability of the machine learning model to generalize.   \n\n****\n**Note:** the second case of feature selection involves applying a selection statistic or hypothesis test multiple times. For large number of features, this process is very likely to lead to false positive and false negative results. This likely outcome is known as the **multiple comparisons problem** in statistics.\n\nTo understand this problem, consider the decision to keep a feature in a model as a hypothesis test. Any hypothesis test has some probability of both a false positive result and a false negative result. Consider a case where there are 40 uninformative features which are excluded from the model with 95% confidence. There will be an approximately 5% chance of accepting a feature which should be rejected. In this case we would expect about 2 uninformative features to be accepted because of these errors. \n\nYou may well ask, if testing features for importance can fail with large numbers of features, what is the alternative? The most general and scalable alternative is to use regularization methods. Consider applying regularization methods to a linear model. In this case, machine learning algorithm learns which features should be weighted highly and which should not. \n****"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the dataset\n\nYou will now apply the aforementioned principles to the bank credit data set. \n\nAs a first step, run the code in the cell below to load the required packages. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nfrom sklearn import feature_selection as fs\nfrom sklearn import metrics\nimport numpy as np\nimport numpy.random as nr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss\nimport math\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, load the preprocessed files containing the features and the labels. The preprocessing includes the following:\n1. Clean missing values.\n2. Aggregate categories of certain categorical variables. \n3. Encode categorical variables as binary dummy variables.\n4. Standardize numeric variables. \n\nExecute the code in the cell below to load the features and labels as numpy arrays for the example. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features = np.array(pd.read_csv('Credit_Features.csv'))\nLabels = np.array(pd.read_csv('Credit_Labels.csv'))\nprint(Features.shape)\nprint(Labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Eliminate low variance features\n\nAs a fist step in selecting features from this dataset you will remove features with low variance. The `VarianceThreshold` function from the scikit-learn `feature_selection` package identifies features with less than some threshold of unique values. For a probability that a feature is unique $p$ the threshold is specified as;\n\n$$Var(x) = p(1-p)$$\n\nIn this case a 80%, or $p=0.8$, threshold is used. \n\nThe `fit_transform` method applies the threshold to the variance of each feature and removes features with variance below the threshold. The `get_support_` attribute shows the `True` and `False` logical for inclusion of each feature. \n\nExecute the code, examine the result, and answer **Question 1** on the course page. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(Features.shape)\n\n## Define the variance threhold and fit the threshold to the feature array. \nsel = fs.VarianceThreshold(threshold=(.8 * (1 - .8)))\nFeatures_reduced = sel.fit_transform(Features)\n\n## Print the support and shape for the transformed features\nprint(sel.get_support())\nprint(Features_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The number of features has been reduced from 35 to 18. Apparently, there are 17 low variance features in the original array. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Select k best features\n\nThe low variance features have been eliminated. But, the question remains, are all these features informative? There are a number of methods used to determine the importance of features. Many machine learning models have specialized methods to determine feature importance specifically intended for those methods. \n\nIn this example, you will use a fairly general and robust method using cross validation. The algorithm is straight forward. Features are recursively removed.  Cross validation is used to find the change in model performance, if any, to determine if a feature should be deleted altogether. \n\nThe code in the cell below performs the following processing:\n1. Create the folds for the cross validation for feature selection. These folds should be independent of any other cross validation performed. \n2. The logistic regression model is defined. \n3. The `RFECV` function from the scikit-learn `feature_selection` package is used to determine which features to retain using a cross validation method. Notice that AUC is used as the model selection metric as the labels are imbalanced. In this case, the default, accuracy is a poor choice. \n4. The RFECV feature selector is fit to the data. \n\nExecute this code and examine the results."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Reshape the Label array\nLabels = Labels.reshape(Labels.shape[0],)\n\n## Set folds for nested cross validation\nnr.seed(988)\nfeature_folds = ms.KFold(n_splits=10, shuffle = True)\n\n## Define the model\nlogistic_mod = linear_model.LogisticRegression(C = 10, class_weight = {0:0.45, 1:0.55}) \n\n## Perform feature selection by CV with high variance features only\nnr.seed(6677)\nselector = fs.RFECV(estimator = logistic_mod, cv = feature_folds,\n                      scoring = 'roc_auc')\nselector = selector.fit(Features_reduced, Labels)\nselector.support_ ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From the support you can see that some features are selected (True) and eliminated (False). \n\nExecute the code below to see the relative ranking of the features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "selector.ranking_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the features which have been selected are shown with a rank of 1. The features eliminated are shown with higher numbers. \n\nThe code in the cell below as uses the `transform` method applies the selector to the feature array. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features_reduced = selector.transform(Features_reduced)\nFeatures_reduced.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The features have been reduced from the 18 high variance features to 16. Two features have been found to be unimportant. \n\nThe code in the cell below creates a plot of AUC (the metric) vs. the number of features. Execute this code. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.title('Mean AUC by number of features')\nplt.ylabel('AUC')\nplt.xlabel('Number of features')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the change in AUC is not that great across a range of features around the 16 selected. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Apply nested cross validation to create model\n\nThe next step is to use nested cross validation to optimize the model hyperparameter and test the model performance. The model is constructed using the features selected. \n\nAs a first step, construct the inside and outside folds for the nested cross validation by running the code in the cell below. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(123)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(321)\noutside = ms.KFold(n_splits=10, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code in the cell below performs the grid search for the optimal model hyperparameter. As before, the scoring metric is AUC.   "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(3456)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\n## Define the logistic regression model\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 1:0.55}) \n\n## Perform the grid search over the parameters\nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\n\n## Fit the cross validated grid search over the data \nclf.fit(Features_reduced, Labels)\n\n## And print the best parameter value\nclf.best_estimator_.C",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The optimal value of the hyperparameter is 1. This parameter is larger than for the same model with all the features. Recalling that the parameter is the inverse of regularization strength, the smaller parameter means the model with fewer features requires less regularization. \n\nTo get a feel for the results of the cross validation execute the code in the cell below and observe the results. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_cv(clf, params_grid, param = 'C'):\n    params = [x for x in params_grid[param]]\n  \n    keys = list(clf.cv_results_.keys())              \n    grid = np.array([clf.cv_results_[key] for key in keys[6:16]])\n    means = np.mean(grid, axis = 0)\n    stds = np.std(grid, axis = 0)\n    print('Performance metrics by parameter')\n    print('Parameter   Mean performance   STD performance')\n    for x,y,z in zip(params, means, stds):\n        print('%8.2f        %6.5f            %6.5f' % (x,y,z))\n    \n    params = [math.log10(x) for x in params]\n    \n    plt.scatter(params * grid.shape[0], grid.flatten())\n    p = plt.scatter(params, means, color = 'red', marker = '+', s = 300)\n    plt.plot(params, np.transpose(grid))\n    plt.title('Performance metric vs. log parameter value\\n from cross validation')\n    plt.xlabel('Log hyperparameter value')\n    plt.ylabel('Performance metric')\n    \nplot_cv(clf, param_grid)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the mean AUCs are within 1 standard deviation of each other. The AUC for the hyperparameter value of 10 is not significantly better than the other values tested. \n\nNow you will perform the outer loop of the nested cross validation by executing the code in the cell below. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The performance metric is not significantly different than for the inner loop of  the cross validation. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test the model\n\nWith the features selected and the optimal hyperparameters estimated, it is time to test the model. the code in the cell below does the following processing;\n1. Split the reduced feature subset of the data into training and test subsets.\n2. Define and fit a model using the optimal hyperparameter. \n\nExecute this code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features_reduced.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features_reduced[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n\n## Define and fit the logistic regression model\nlogistic_mod = linear_model.LogisticRegression(C = 1, class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, execute the code in the cell below to score the model and display a sample of the resulting probabilities. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\nprobabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])\nscores = score_model(probabilities, 0.3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With the model scored, execute the code in the cell below to display performance metrics for the model. Then, answer **Question 2** on the course page."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\ndef plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()    \n        \nprint_metrics(y_test, probabilities, 0.3)    \nplot_auc(y_test, probabilities)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "At first glance, these performance metrics look quite good. Notice however, that the AUC is much larger than achieved with cross validation. This indicates that these results are overly optimistic, a common situation when a single split is used to evaluate a model. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Summary\n\nIn this lab you have performed two types of feature selection:\n1. Eliminating low variance features, which by their nature cannot be highly informative since they contain a high fraction of the same value.\n2. Using recursive feature elimination, a cross validation technique for identifying uninformative features. \n\nWith a reduced feature set less regularization was required for the model. This is expected since the most uninformative features have already been eliminated. It should be noted that for large numbers of features, these types of feature elimination algorithms should not be expected to give good generalization performance as a result of the multiple comparisons problem. In these cases, stronger regularization is a better approach. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}